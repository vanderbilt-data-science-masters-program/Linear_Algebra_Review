---
title: "Linear Algebra R Examples"
date: "8/16/2019"
author: "Megan Hollister"
output:
  html_document: 
    code_folding: hide
    theme: cosmo
    toc: yes
    toc_depth: 5
    toc_float: true
editor_options: 
  chunk_output_type: inline
---

### Email: [megan.c.hollister@vanderbilt.edu]()

## Description: 

This supplement is meant to instruct and clarify applications of the key concepts of linear algebra.

```{r include=FALSE, message=FALSE, warning=FALSE}
# install.packages("MASS")
# install.packages("knitr")
# install.packages("matlib")
# install.packages("magrittr")
# install.packages("pcaPP")
# install.packages("devtools")

library(devtools)
# install_github("vqv/ggbiplot")

library(MASS)
library(knitr)
library(matlib)
library(magrittr)
library(pcaPP)
library(ggbiplot)

example.data <- mtcars
```

# 1. Introduction to Matrices/Vectors
## a. Definitions and Terms

+ Create a vector with 3 elements.

$$v=\begin{pmatrix} 
1 \\
7 \\
3
\end{pmatrix}$$

```{r}
v = c(1,7,3)

# The exact same when wrapped in as.vector()
v = as.vector(c(1,7,3))

# Or can create a 1 column matrix
v = matrix(c(1,7,3))
```

+ Create a matrix with 6 elements filling by rows. 

$$A=\begin{bmatrix} 
2 & 4 & 3 \\
1 & 5 & 7 
\end{bmatrix}$$

```{r}
A = matrix( 
  c(2, 4, 3, 1, 5, 7), # the data elements 
  nrow=2,              # number of rows 
  ncol=3,              # number of columns 
  byrow = TRUE)        # fill matrix by rows (default)
```

+ Create a matrix with the same 6 elements filling by columns. 

$$B=\begin{bmatrix} 
2 & 1  \\
4 & 5 \\
3 & 7
\end{bmatrix}$$

```{r}
B = matrix( 
  c(2, 4, 3, 1, 5, 7), # the data elements 
  nrow=3,              # number of rows 
  ncol=2,              # number of columns 
  byrow = FALSE)       # fill matrix by columns
```

+ Create a square matrix with 9 elements. 

$$C=\begin{bmatrix} 
25 & 4 & 31  \\
1 & 52 & 7 \\
81 & 22 & 13
\end{bmatrix}$$

```{r}
C = matrix( 
  c(25, 4, 31, 1, 52, 7, 81, 22, 13), # the data elements 
  nrow=3,              # number of rows 
  byrow = TRUE)        # fill matrix by rows (default)
```

+ Create a identity matrix with 9 elements. 

$$I=\begin{bmatrix} 
1 &0&0  \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}$$

```{r}
I = diag(1,      # The element(s) on the diagonal of matrix
         nrow=3) # number of rows 
                 # (assumes same number of columns or square)
```

+ Create a unit matrix with 9 elements. 

$$U =\begin{bmatrix} 
1 &1&1  \\
1 & 1 & 1 \\
1 & 1 & 1
\end{bmatrix}$$

```{r}
U = matrix(1,
           nrow=3,   # number of rows
           ncol=3)   # number of columns
```

+ Label the rows and columns of the matrix. 

$$A=\begin{matrix}
& col1 & col2 & col3 \\
row1 & 2 & 4 & 3 \\
row2 & 1 & 5 & 7 
\end{matrix}$$

```{r}
dimnames(A) = list( 
   c("row1", "row2"),         # row names 
   c("col1", "col2", "col3")) # column names 

# Check the row and column labels (in list format)
# labels(A)
```

+ Transpose a matrix. 

$$A^T=\begin{matrix}
& row1 & row2  \\
col1 & 2 & 1 \\
col2 & 4 & 5 \\
col3 & 3 & 7  
\end{matrix}$$

```{r}
At <- t(A)
```

## b. Operations and Properties

Multiplication:

+ Multiply every element in matrix with a constant. 

$$5\times B=\begin{bmatrix}
10&5 \\
20&25 \\
15&35 \\
\end{bmatrix}$$

```{r}
k = 5

kB = k*B
```

+ Multiply each column in matrix by different constant. 

$$\begin{bmatrix} 
0\times B_{col1} & 2\times B_{col2} \\
\end{bmatrix}= \begin{bmatrix}
0&2 \\
0&10 \\
0&14 \\
\end{bmatrix}$$

```{r}
# WRONG
# c(0,2)*B
# t(c(0,2))*B
# Error: non-conformable arrays

# 3 CORRECT methods
B.scaled = B%*%diag(c(0,2))

B.scaled = t(apply( B , 1 , function(x) x*c(0,2)))

B.scaled = sweep( B , 2 , c(0,2),'*')
```

+ Multiply vector and matrix using matrix multiplcation. 

$$v^T*B=\begin{bmatrix}
39&57 \\
\end{bmatrix}$$
$$B^T*v=\begin{bmatrix}
39 \\
57 \\
\end{bmatrix}$$

```{r}
vB = t(v)%*%B

Bv = t(B)%*%v
```

+ Multiply 2 matrices using matrix multiplcation. 

$$AB=\begin{bmatrix}
29&43 \\
43&75 \\
\end{bmatrix}$$

$$BA=\begin{bmatrix}
5&13&13 \\
13&41&47 \\
13&47&58 \\
\end{bmatrix}$$

```{r}
# Using element-wise mulitplication( * ): 
# A*B
# Error: non-conformable arrays 

AB = A%*%B #returns a 2x2 matrix

BA = B%*%A #returns a 3x3 matrix
```

+ Multiply 3 matrices together.

$$BAC = \begin{bmatrix}
1191&982&415 \\
4173&3218&1301 \\
5070&3772&1486 \\
\end{bmatrix}$$

```{r}
BAC = B%*%A%*%C
```

Addition:

+ Add constant (-3) to every element in a matrix (C). 

$$C^{*} = \begin{bmatrix}
22&1&28 \\
-2&49&4 \\
78&19&10 \\
\end{bmatrix}$$

```{r}
C.centered = C-3
```

+ Add different constant to each column of a matrix. 

$$\hat{C}=\begin{bmatrix} 
C_{col1}+0 & C_{col2}-5 & C_{col3}+7 \\
\end{bmatrix}= \begin{bmatrix}
25&-1&38 \\
1&47&14 \\
81&17&20 \\
\end{bmatrix}$$

```{r}
C.hat = C + matrix(c(0,-5,7), nrow=3, ncol=3, byrow=TRUE)

C.hat = t(apply( C , 1 , function(x) x+c(0,-5,7)))

C.hat = sweep( C , 2 , c(0,-5,7),'+')
```

```{r}
# Change A to a different variable type in order to use "mutate" function

A <- as.data.frame(A)
#A <- as.matrix(A)

A %<>% mutate(col1=col1*8)
```

# 2. Systems of Linear Equations 
## a. Gaussian Elmination 

Solve the folowing for $x$:

$$Dx=e$$
$$D=\begin{bmatrix}
1&1&1 \\
2&3&7 \\
-1&0&-9 
\end{bmatrix}$$

$$e=\begin{pmatrix}
3 \\
0 \\
17 
\end{pmatrix}$$

Solution:
$$x=\begin{pmatrix}
1 \\
4 \\
-2 
\end{pmatrix}$$

```{r}
D <- matrix(c(1,1,1,2,3,7,1,0,9), nrow=3, byrow = TRUE)

e <- c(3,0,-17)

# gaussianElimination(D,e, verbose=TRUE, fractions=TRUE)

D <- matrix(c(1,1,3,-2), nrow=2, byrow = TRUE)

e <- c(3,4)

# gaussianElimination(D,e, verbose=TRUE, fractions=TRUE)
```

## b. Linear Independence 

Are the following vectors linearly independent?

$$v_1 = \begin{pmatrix}
2 \\
5 \\
3 
\end{pmatrix}$$

$$v_2 = \begin{pmatrix}
1 \\
1 \\
1 
\end{pmatrix}$$

$$v_3 = \begin{pmatrix}
4 \\
-2 \\
0 
\end{pmatrix}$$


```{r}
V <- matrix(c(2, 5, 3,1, 1, 1,4,-2,0), nrow=3, byrow = TRUE)

# gaussianElimination(V, verbose=TRUE, fractions=TRUE)
```

One way to answer this question is find the solution to:

$$k_1v_1+k_2v_2+k_3v_3=0$$

Through gaussian elimination we see the only solution is $k_1 = k_2 = k_3 = 0$, which proves that the given vectors are linearly independent. 

Solving Systems of Equations Examples

+ Three consistent equations. 

$$x_1 - x_2  =  2 $$
$$2x_1 + 2x_2  =  1 $$
$$3x_1 + x_2  =  3 $$
Solution:

```{r}
A <- matrix(c(1,2,3, -1, 2, 1), 3, 2)
b <- c(2,1,3)

# Print the system of equations
showEqn(A,b)

# Test for consistency, are the ranks equal?
R(A)==R(cbind(A,b))

# Solve for the solution
Solve(A,b, fractions = TRUE)
```

From the plot we can see all 3 equations have a consistent solution. 
```{r}
plotEqn(A,b)
```

+ Three inconsistent equations. 

$$x_1 - x_2  =  2 $$
$$2x_1 + 2x_2  =  1 $$
$$3x_1 + x_2  =  6 $$

Solution:
```{r}
b.new <- c(2,1,6)

# Print the system of equations
showEqn(A,b.new)

# Test for consistency, are the ranks equal?
R(A)==R(cbind(A,b.new))

# Solve for the solution, see 0=-3 which is FALSE
Solve(A,b.new, fractions = TRUE)
```

From the plot we can see that each pair of equations has a solution, but all three don't have a consistent solution.

```{r}
plotEqn(A,b.new)
```

## c. Inverses

$$F=\begin{bmatrix}
2&1&0 \\
-2&-1&3 \\
0&1&-2 \\
\end{bmatrix}$$

$$F^{-1}=\begin{bmatrix}
\frac{1}{6}&-\frac{1}{3}&-\frac{1}{2} \\
\frac{2}{3}&\frac{2}{3}&1 \\
\frac{1}{3}&\frac{1}{3}&0 \\
\end{bmatrix}$$

```{r}
F <- matrix(c(2,-2,0,1,-1,1,0,3,-2), nrow=3)

# Step-by-step process to solve for inverse
# gaussianElimination(F,diag(1,nrow=3), verbose=TRUE, fractions=TRUE)
```

```{r}
FI <- fractions(inv(F))

FI <- fractions(Inverse(F))

FI <- fractions(solve(F))
```

Show the following hold: 

+ `inv(inv(A)) = A`

+ `inv(t(A)) = t(inv(A))`

+ `inv( k*A ) = (1/k) * inv(A)`

+ `inv(A %*% B) = inv(B) %*% inv(A)`

+ `inv(A) %*% A = diag(nrow(A))`

## d. Norm and Trace and  Determinant

$$norm(C)=107$$
$$det(C)=-114624$$

$$tr(C)=90$$

How to code:

```{r}
# Default norm type is "one" or maximum absolute column sum
# For other norm types see documentation
norm.C <- norm(C)

det.C <- det(C)

trace.C = sum(diag(C))

cov.C <- cov(C)

cor.C <- cor(v,2*v)

# Not perpendicular
t(v)%*%c(0,2,4) == 0
```

Some properties include:

+ `trace(A) == trace(t(A))`

+ `trace(alpha * A) == alpha * trace(A)`

+ `trace(A %*% B) == trace(B %*% A)`

+ `trace(A) == trace(crossprod(crossprod(B,A),solve(B)))`

+ `det(inv(A))=1/det(A)=inv(det(A))`

+ `det(A) != 0, so inverse exists`

# 3. Additional Topics

## a. The Moore Penrose Pseudoinverse

$$G_C=\begin{bmatrix}
-0.005&-0.005&0.014 \\
-0.005&0.019&0.001 \\
0.037&0.002&-0.011 \\
\end{bmatrix}$$

How to code:

```{r}
G.C <- ginv(C)
```

## b. Random Functions

See code: 

```{r}
# matrix of logical variables returned
upC <- upper.tri(C, diag=TRUE)
lowC <- lower.tri(C)

# return the values in the upper traingle
C[upC]

# replace the values in the lower traingle with 0's
C[lowC] <- 0

#logical variable returned
sym.C <- is_symmetric_matrix(C) 
```

How to code eigenvalues and Singular value decomposition:

```{r}
eig.C <- eigen(C)

# eig.C$values
# eig.C$vectors

C.svd <- svd(C)
```

## c. Linear Regression

This is a very simple example. You will learn a lot more in class this semester about how to build and interpret linear models. 

Model 1

Here we fit a linear model that predicts 1974 car fuel consumption, more specifically miles per gallon, using car weight. We can see in the graph that as car weight increases mpg decreases. Therefore there is a negative trend between weight and mpg in this sample. 

$$Y = \beta_0+\beta_1\times x_1$$
$$Y = X\beta$$

$$X=\begin{bmatrix}
1 & x_1 \\
\end{bmatrix}=\begin{bmatrix}
1 & wt \\
\end{bmatrix}$$

$$\beta=\begin{bmatrix}
\beta_0 & \beta_1 \\
\end{bmatrix}$$

$$\text{mpg} = \beta_0+\beta_1\times \text{wt}$$

```{r}
example.data <-mtcars 

lin.model <- lm(mpg ~ wt, data=example.data)

summary(lin.model)
```


```{r}
#number of cars in dataset
n = length(example.data$wt)

X <- matrix(c(rep(1,n),example.data$wt),ncol=2)

Y <- example.data$mpg
```

Coefficients:
$$\beta=(X^TX)^{-1}X^TY$$
Predictions:
$$\hat{Y}=X(X^TX)^{-1}X^TY$$

```{r}
# Same result of coefficients:
#Matrix multiplication for linear regression
Y.coef <- solve(t(X)%*%X)%*%t(X)%*%Y
coef(lin.model)

# Use the fitted model to predict the mpg for each car (where they fall on the fit line)
Y.pred <- X%*%Y.coef
predict(lin.model)
```

```{r}
plot(example.data$wt, example.data$mpg, main="Car Weight vs. Fuel Consumption Plot",xlab="Weight", ylab="mpg")
abline(lm(mpg~wt,data=example.data), col="red")
```

Model 2

Here we fit a linear model to predict miles per gallon using car weight and a linear combination of weight. This will show how the `lm()` function will drop variables that are perfectly correlated. Here we force the linear combination `new` to be incorporated in the model and use matrix multiplcation to calculate the new coefficients, $\beta$, and he predictions. 

$$mpg = \beta_0+\beta_1*wt+\beta_2*new$$
$$= \beta_0+\beta_1*wt+\beta_2*(2wt-70)$$

```{r}
#Create new variable that is a linear combination of weight
example.data$new <- 2*example.data$wt-70
```

```{r}
lin.model2 <- lm(mpg ~ wt+new,data=example.data)

#See that lm has dropped the new variable by giving it a NA coefficient
#coef(lin.model2)
summary(lin.model2)

X2 <- matrix(c(rep(1,length(example.data$wt)),example.data$wt, example.data$new),ncol=3)
```

```{r}
# Unequal ranks, singular
R(X2)==R(cbind(X2,Y))

#The inverse doesnt exist because with new we have a singular matrix
# solve(t(X2)%*%X2)
```

```{r}
# Because singular we cannot use regular inverse
# So instead we use generalized inverse
g.coef <- ginv(t(X2)%*%X2)%*%t(X2)%*%Y
g.pred <- X2%*%g.coef

# Now see that both the first linear model
# And the second linear model including "new"
# Get the exact same predictions

# See how many predictions are equal
# g.pred==Y.pred
sum(g.pred==Y.pred)
# Percent equal
sum(g.pred==Y.pred)/n

# Now round each of the predictions to see if that affects the equality
sum(round(g.pred,8)==round(Y.pred,8))
# Percent equal when rounded
sum(round(g.pred,8)==round(Y.pred,8))/n
```

```{r}
plot(example.data$wt,example.data$mpg,pch=20,col="black", main="Model 1 vs. Model 2 Predictions", xlab="Weight", ylab="mpg")
lines(example.data$wt,Y.pred,lty=1,lwd=8,col="blue")
lines(example.data$wt,g.pred,lty=1,lwd=2,col="green")
```

## d. Principal Component Analysis

`mtcars`: This dataset consists of data on 32 models of car, taken from an American motoring magazine (1974 Motor Trend magazine). For each car, you have 11 features, expressed in varying units. 

We will consider only the numeric variables for the PCA (not the categorical ones `vs` and `am`). We fit a PCA object to the scaled and centered data. Each of these 9 components explains a percentage of the total variation in the dataset. 

"PC1 explains 63% of the total variance, which means that nearly two-thirds of the information in the dataset (9 variables) can be encapsulated by just that one Principal Component. PC2 explains 23% of the variance. So, by knowing the position of a sample in relation to just PC1 and PC2, you can get a very accurate view on where it stands in relation to other samples, as just PC1 and PC2 can explain 86% of the variance."

```{r}
mtcars.pca <- prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE)

summary(mtcars.pca)
```

From the plot of PC1 vs. PC2 we see variables `hp`, `cyl`, and `disp` all contribute to PC1, with higher values in those variables moving the samples to the right on this plot. This lets you see how the data points relate to the axes, but it's not very informative without knowing which point corresponds to which sample or car.

```{r}
ggbiplot(mtcars.pca)
```

Label each point on the graph with the car names:

```{r}
ggbiplot(mtcars.pca, labels=rownames(mtcars))
```

In order to interpret these results we compare groups. Here we create a `mtcars.country` variables to represent the country of origin of the car. We will also circle these groups on the graph:

"We see that the American cars are characterized by high values for `cyl`, `disp`, and `wt`. Japanese cars, on the other hand, are characterized by high `mpg`. European cars are somewhat in the middle and less tightly clustered than either group."

```{r}
mtcars.country <- c(rep("Japan", 3), rep("US",4), rep("Europe", 7),rep("US",3), "Europe", rep("Japan", 3), rep("US",4), rep("Europe", 3), "US", rep("Europe", 3))

ggbiplot(mtcars.pca,ellipse=TRUE,  labels=rownames(mtcars), groups=mtcars.country)
```

Now lets look at PC3 vs. PC4. The trends are less distinct here but we expected that because of the low percent of explained variance from each component. 

```{r}
ggbiplot(mtcars.pca,ellipse=TRUE,choices=c(3,4),   labels=rownames(mtcars), groups=mtcars.country)
```


# Sources

+ [Overview of All Topics](https://medium.com/@rathi.ankit/linear-algebra-for-data-science-a9648b9daee0)
 
+ [Inverse Example](https://cran.r-project.org/web/packages/matlib/vignettes/inv-ex1.html)

+ [Trace Properties](https://rpubs.com/aaronsc32/matrix-trace)

+ [Solving Linear Equations](https://cran.r-project.org/web/packages/matlib/vignettes/linear-equations.html)

+ [SVD Example](https://stats.idre.ucla.edu/r/codefragments/svd_demos/)

+ [Linear Regrssion Example](http://r-statistics.co/Linear-Regression.html)

+ [PCA Example](https://www.datacamp.com/community/tutorials/pca-analysis-r)

+ [More detailed explanation of PCA](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.12-Example-Principal-Components-Analysis/?source=post_page-----a9648b9daee0----------------------)


